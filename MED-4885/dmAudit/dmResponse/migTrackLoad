package ccx.dmResponse

import ccx.medcompass.controller.Util

import com.typesafe.config.ConfigFactory
import java.io.PrintWriter
import java.io.StringWriter
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions._
import com.typesafe.config.ConfigFactory
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import ccx.medcompass.Utils.Global.getCurrentDate
import org.apache.spark.sql.expressions.Window

class migTrackLoad extends TRACKStrategy with Util with trackTrait{

  override def loadData(file_Name: String) {

    try {
      val spark = getSparkSession()
      import spark.implicits._

      val filename = file_Name
      logger.info("### Loading Response File => " + filename)

     
      var MIGRATION_TRACKING_MAX = 0
      

      logger.info("MIGRATION_TRACKING_MAX Value => " + MIGRATION_TRACKING_MAX)

      val schema = new StructType()
        .add("SUMM_MSG_TYPE_DESC", StringType, true)
        .add("SUMM_ERROR_MSG_TXT", StringType, true)
        .add("SUMM_ERROR_OBJ", StringType, true)

      var responseDf = spark.read.format("csv")
        .option("header", "true")
        .schema(schema)
        .csv(ConfigFactory.load.getString("migration.tracking.csvFile.hdfsPath").concat("/").concat(filename))
        
         responseDf = responseDf.withColumn("FILE_NM", lit(filename))
     // responseDf.show(200,false)

      //monotonically_increasing_id is not a sequential ID and it is distributed which performs according to partition of the data
      //so thats why switching to Index (row_number) for sequence generation
      responseDf = responseDf.withColumn("index", row_number().over(Window.orderBy(col("FILE_NM").asc)))
       responseDf = responseDf.withColumn("MIGRATION_TRKNG_ID", col("index") + MIGRATION_TRACKING_MAX)
     
     //responseDf.show(200,false)
     responseDf = responseDf.select("MIGRATION_TRKNG_ID", "FILE_NM", "SUMM_MSG_TYPE_DESC", "SUMM_ERROR_MSG_TXT", "SUMM_ERROR_OBJ")
     val FiletrackingTbl = ConfigFactory.load.getString("file-tracking-tbl")
          val tempTblNm = FiletrackingTbl + "_stg"
          val runsummaryTblNm= FiletrackingTbl + "_runsummary"
          val runsummaryFileTblNm= FiletrackingTbl + "_runsummaryfiles"
          val runsummaryerrTblNm= FiletrackingTbl + "_error"
          val runsummaryerrtemTblNm= FiletrackingTbl + "_errorstg"
      val SAtrackingTbl = ConfigFactory.load.getString("ServiceAuth-tracking-tbl")
      val SAtempTblNm = SAtrackingTbl + "_stg"
      val SAErrtempTblNm = SAtrackingTbl + "_error"
         val PrvtrackingTbl = ConfigFactory.load.getString("Provider-tracking-tbl")
      val PrvtempTblNm = PrvtrackingTbl + "_stg"
       val PrvErrtempTblNm = PrvtrackingTbl + "_error"
        val PrvtrctempTblNm = PrvtrackingTbl + "_track"
        val PrvsucctempTblNm = PrvtrackingTbl + "_succ"
      val MemtrackingTbl = ConfigFactory.load.getString("Member-tracking-tbl")
      val MemtempTblNm = MemtrackingTbl + "_stg"
       val MemErrtempTblNm = MemtrackingTbl + "_error"
       val MemtrctempTblNm = MemtrackingTbl + "_track"
        val MemsucctempTblNm = MemtrackingTbl + "_succ"
        
      logger.info("### Writing response file to Migration Tracking tbl")
      spark.sql(s"drop table if exists $runsummaryTblNm")
      responseDf.write.mode("overwrite").saveAsTable(s"$runsummaryTblNm")

      logger.info("### filesummary,error###")
      var filesummaryErrorDF=spark.sql(s"SELECT FILE_NM,SUMM_MSG_TYPE_DESC,regexp_replace(split(summ_error_msg_txt,'[ ]')[6], ':','') as extract_file_name,split(summ_error_msg_txt,'[ ]')[1] as object_name from $runsummaryTblNm where SUMM_MSG_TYPE_DESC='FileSummary'")
      filesummaryErrorDF.show(200,false)
      
      logger.info("### filesummary temp table creation###")
    var filesummaryDF=spark.sql(s"SELECT distinct regexp_replace(split(summ_error_msg_txt,'[ ]')[6], ':','') as extract_file_name,FILE_NM from $runsummaryTblNm where SUMM_MSG_TYPE_DESC='FileSummary'")
      //filesummaryDF.distinct.createOrReplaceTempView("FileSummary")
    spark.sql(s"drop table if exists $runsummaryFileTblNm")
     filesummaryDF.write.mode("overwrite").saveAsTable(s"$runsummaryFileTblNm")
 
      
      //filesummaryDF.write.mode("overwrite").saveAsTable("prod.FileSummary")
      logger.info("### filesummary table update for success status...### ")
      //spark.sql(s"drop table if exists prod.dm_filesummary_tracking1")
      //spark.sql(s"create table prod.dm_filesummary_tracking_1 like prod.dm_file_summary_tracking")
      //spark.sql(s"create table prod.dm_filesummary_tracking_1 like prod.dm_file_tracking")
      //spark.sql(s"INSERT INTO table prod.dm_filesummary_tracking1  SELECT  a.extract_file_name as extract_file_name,a.objectname as objectname,a.count as count,CASE WHEN B.extract_file_name IS NOT NULL THEN 'Processed' ELSE a.file_status end AS Status FROM  prod.dm_file_summary_tracking  A LEFT OUTER JOIN  prod.dm_runsummary_files B ON A.extract_file_name = B.extract_file_name") 
      //spark.sql(s"INSERT INTO table prod.dm_filesummary_tracking1  SELECT  a.extract_file_name as extract_file_name,a.objectname as objectname,a.count as count,CASE WHEN B.extract_file_name IS NOT NULL THEN 'Processed' ELSE A.file_status end AS Status FROM  prod.dm_file_summary_tracking  A LEFT OUTER JOIN  prod.dm_filesummary_tracking1 B ON A.extract_file_name = B.extract_file_name where b.extract_file_name is null")
     // var filesummaryupdatedDF=spark.sql(s"insert overwrite table prod.dm_file_summary_tracking select * from prod.dm_filesummary_tracking_1") 
      spark.sql(s"drop table if exists $tempTblNm")
      spark.sql(s"create table $tempTblNm like $FiletrackingTbl")
      spark.sql(s"INSERT INTO table $tempTblNm  SELECT a.fileid as fileid,a.extract_file_name as extract_file_name,a.total_no_records as total_no_records,a.total_no_of_unique_record as total_no_of_unique_record,CASE WHEN B.extract_file_name IS NOT NULL THEN 'success' ELSE a.status end AS status,CASE WHEN B.extract_file_name IS NOT NULL THEN b.FILE_NM  ELSE a.runsummaryfilename end as runsummaryfilename  FROM $FiletrackingTbl  A LEFT OUTER JOIN  $runsummaryFileTblNm B ON A.extract_file_name = B.extract_file_name") 
      spark.sql(s"INSERT INTO table $tempTblNm   SELECT a.fileid as fileid,a.extract_file_name as extract_file_name,a.total_no_records as total_no_records,a.total_no_of_unique_record as total_no_of_unique_record, A.status as Status,a.runsummaryfilename as runsummaryfilename FROM  $FiletrackingTbl  A LEFT OUTER JOIN  $runsummaryFileTblNm B ON A.extract_file_name = B.extract_file_name where b.extract_file_name is null")
      var filesummaryupdatedDF=spark.sql(s"insert overwrite table $FiletrackingTbl select distinct * from $tempTblNm")
      //filesummaryupdatedDF.show(200,false)
      //spark.sql(s"select * from prod.dm_filesummary_tracking_1").show(200,false)
      //var fdf=spark.sql(s"set hive.support.concurrency = true;set hive.enforce.bucketing = true;set hive.exec.dynamic.partition.mode = nonstrict;set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;UPDATE prod.dm_file_summary_tracking_test set file_status='Processed' from filesummary a join prod.dm_file_summary_tracking_test b where a.extract_file_name =b.extract_file_name)")
      
      logger.info("### Error Merge into the Tracking table ###")
      spark.sql(s"drop table if exists $runsummaryerrTblNm")
      var ErrorsummaryDF=spark.sql(s"select SUMM_ERROR_MSG_TXT, SUMM_ERROR_OBJ from $runsummaryTblNm where SUMM_MSG_TYPE_DESC ='Error'")
     // var ErrorSummaryDF=spark.sql("create external table prod.dm_file_error_summary(summary string) ROW FORMAT delimited fields terminated by '#'  STORED AS TEXTFILE LOCATION 'migration.tracking.csvFile.hdfsPath' TBLPROPERTIES ('textinputformat.record.delimiter'='#')")
      ErrorsummaryDF.write.mode("overwrite").saveAsTable(s"$runsummaryerrTblNm")
      spark.sql(s"drop table if exists $runsummaryerrtemTblNm")
      var ErrormemDF=spark.sql(s"select a.gdfid,concat_ws(',', COLLECT_LIST(b.summ_error_obj)) as errorcd,concat_ws(',', COLLECT_LIST(b.SUMM_ERROR_MSG_TXT)) as reasoncd from $MemtrackingTbl a, $runsummaryerrTblNm b where b.summ_error_obj like '%'||a.gdfid||'%' and a.status ='sent' group by a.gdfid")
      ErrormemDF.write.mode("overwrite").saveAsTable(s"$MemErrtempTblNm")
      var ErrorproDF=spark.sql(s"select a.gdfid,concat_ws(',', COLLECT_LIST(b.summ_error_obj)) as errorcd,concat_ws(',', COLLECT_LIST(b.SUMM_ERROR_MSG_TXT)) as reasoncd from $PrvtrackingTbl a, $runsummaryerrTblNm b where b.summ_error_obj like '%'||a.gdfid||'%' and a.status ='sent' group by a.gdfid")
      ErrorproDF.write.mode("overwrite").saveAsTable(s"$PrvErrtempTblNm")
     // ErrorDF.show(200,false)
      var ErrorsaDF=spark.sql(s"select a.caseid ,concat_ws(',', COLLECT_LIST(b.summ_error_obj)) as errorcd,concat_ws(',', COLLECT_LIST(b.SUMM_ERROR_MSG_TXT)) as reasoncd from $SAtrackingTbl a, $runsummaryerrTblNm b where b.summ_error_obj like '%'||a.caseid||'%' and a.status ='sent' group by a.caseid")
     ErrorsaDF.write.mode("overwrite").saveAsTable(s"$SAErrtempTblNm")
      
      logger.info("### Tracking tables update ###")
      if (!isDataFrameEmpty(ErrormemDF)) {
     // var trackDF=spark.sql(s"select MIGRATION_TRKNG_ID, case when split(summ_error_msg_txt,'[\ ]')[1] like '%Member%' then split(summ_error_msg_txt,'[\ ]')[2] else,");
     // var trackingDF=spark.sql(s"set hive.support.concurrency = true;set hive.enforce.bucketing = true;set hive.exec.dynamic.partition.mode = nonstrict;set hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;UPDATE prod.dm_member_tracking_test set file_status='Processed' from  prod.dm_file_summary_tracking_test b where a.extract_file_name =b.extract_file_name)")
      logger.info("### Member Tracking table update###")
      spark.sql(s"drop table if exists $MemtempTblNm")
      spark.sql(s"create table $MemtempTblNm like $MemtrackingTbl")
       spark.sql(s"drop table if exists  $MemtrctempTblNm")
      var MemDf=spark.sql(s"create table $MemtrctempTblNm as select gdfid from $MemtrackingTbl where status ='sent' ")
      spark.sql(s"SET hive.exec.dynamic.partition = true")
      spark.sql(s"SET hive.exec.dynamic.partition.mode = nonstrict ")
      spark.sql(s"SET hive.exec.max.dynamic.partitions.pernode = 400")
      spark.sql(s"INSERT INTO table $MemtempTblNm  SELECT a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime,'failed' AS status,b.reasoncd as reasoncd, b.errorcd  as errorrcd,a.run_date as run_date FROM $MemErrtempTblNm  b LEFT OUTER JOIN  $MemtrackingTbl a ON A.gdfid = B.gdfid") 
       spark.sql(s"INSERT INTO table $MemtempTblNm  SELECT a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime, a.status AS status,'' as reasoncd, '' as errorrcd,a.run_date as run_date FROM $MemtrackingTbl a LEFT OUTER JOIN  $MemErrtempTblNm b ON A.gdfid = B.gdfid where b.gdfid is null and a.status not in ('sent') ")
      spark.sql(s"INSERT INTO table $MemtempTblNm SELECT a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime,'success' AS status,'' as reasoncd, '' as errorrcd,a.run_date as run_date FROM  $MemtrackingTbl a LEFT OUTER JOIN  $MemErrtempTblNm b ON A.gdfid = B.gdfid where b.gdfid is null and a.status='sent'")
      var ErrormupdatedDF=spark.sql(s"insert overwrite table $MemtrackingTbl select * from $MemtempTblNm")
      //ErrorupdatedDF.show(200,false)
      logger.info(s"### Member Tracking table updated####")
      spark.sql(s"drop table if exists  $MemsucctempTblNm")
      logger.info(s"### UDH file extraction for Member started ####")
        logger.info(s"### UDH file extraction for Header Member started ####")
        var df =spark.sql(s"select count(*) from $MemtrackingTbl ")
      df =df.withColumn("tranTimeStamp", to_utc_timestamp(current_timestamp(), "EST"))
      val timestamp=df.select(s"tranTimeStamp")
       // val headerDf=df.map(x=> ("HDR"+"|"+"DM_UDH_MCResp_Member_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".","")+".txt"+"|"+x(1).toString().replace(" ","T")+"|" +"DATA-MIGRATION"+"|"+"UDH"+"|"+"MEMBER")).toDF("value").withColumn("index", lit("1"))
          logger.info(s"### UDH file extraction for Header Member ends ####")
          logger.info(s"### UDH file extraction for Member data started ####")
      var MemSuccessDf=spark.sql(s"create table $MemsucctempTblNm as select gdfid as successgdfid from $MemtrackingTbl where status ='success' ")
      //ErrorupdatedDF.show(200,false)
      //var MemSuccessDf=spark.sql(s"select gdfid as successgdfid from prod.dm_member_tracking_divya where status ='success' ")
     // var MemgdfidDf=MemSuccessDf.alias("d1").join(MemDf.alias("d2"),col("d1.successgdfid")=== col("d2.gdfid"),"inner")
      var MemgdfidDf=spark.sql(s"select a.successgdfid from  $MemsucctempTblNm a inner join $MemtrctempTblNm b on a.successgdfid=b.gdfid")
      MemgdfidDf.show(200,false)
     var dataMemDf= MemgdfidDf.select(s"successgdfid")
     
     var reDf=responseDf.select("FILE_NM")
     var  Dm1Df=reDf.withColumn("vendorResponse",expr("substring(FILE_NM,16,12)")).drop("FILE_NM")
     val tidf=Dm1Df.withColumn("datetype_timestamp",to_utc_timestamp(to_timestamp(col("vendorResponse"),"yyyyMMddHHmm"),"EST"))
      // val tidf =Dm1Df.withColumn("vendorResponsetimestamp", to_utc_timestamp(col("vendorResponse"), "EST")).drop("vendorResponse")
       tidf.show(200,false)
       
       val dmdsdf =tidf.map(x=> x(1).toString().replace(" ","T")).toDF("vendortimestamp")
          val dmd=dmdsdf.select("vendortimestamp").collectAsList().get(0).get(0).toString()
      // val ven=dmd
      //    dataMemDf=Dm1Df.select("vendorResponsetimestamp")
          //val tme=dmd
       dataMemDf=dataMemDf.withColumn("vendorResponsetimestamp", lit(s"$dmd"))
          dataMemDf=dataMemDf.withColumn("vendorName",lit("MEDCOMPASS")).withColumn("vendorRecordId",lit("")).withColumn("recordtype",lit("DTL"))
         // val dataDf=dataMemDf.select(s"vendorResponsetimestamp").toString()
          //val dataDF=dataMemDf.map(x => (x(5)+"|"+ x(2)+"|"+x(0)+"|"+ x(3).toString().replace(" ","T")+"|"+ x(4))).toDF("value").withColumn("index", lit("2"))
          //  val totalRows = dataDF.count()
          logger.info(s"### UDH file extraction for Member data ended ####")
      logger.info(s"### UDH file extraction for trailer Member started ####")
           val trailer =MemgdfidDf.count().toString()
           var trailer1Df = spark.sql(s"select count(*) as cnt from $MemtrackingTbl where status ='success' ")
      trailer1Df=trailer1Df.withColumn("recordType",lit("TRL")).withColumn("count",lit(trailer)).drop("cnt")
     // val trailerDf =trailer1Df.map(x=>(x(0)+"|"+x(1))).toDF("value").withColumn("index", lit("3"))
        logger.info(s"### UDH file extraction for trailer Member ends ####")
         logger.info(s"### UDH file extraction merge starts ####")
          val headerDf=df.map(x=> ("HDR"+"|"+"DM_UDH_MCResp_Member_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".","")+".txt"+"|"+x(1).toString().replace(" ","T")+"|" +"DATA-MIGRATION"+"|"+"UDH"+"|"+"MEMBER")).toDF("value").withColumn("index", lit("1"))
         headerDf.show(200,false)
           val filenameDf=df.map(x=> ("HDR","DM_UDH_MCResp_Member_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".",""),x(1).toString().replace(" ","T"),"DATA-MIGRATION","UDH","MEMBER"))
          val file_name=filenameDf.select("_2").collectAsList().get(0).get(0).toString()
           
           val sedf= dataMemDf.select("recordtype","vendorName","successgdfid","vendorResponsetimestamp","vendorRecordId")
         //  val dataDF=sedf.map(x =>(x(0)+"|"+ x(1)+"|"+x(2)+"|"+ x(3)+"|"x(4)))
           val allColumns = sedf.columns.map(x => col(x))
         val dataDF=sedf.withColumn("Final", concat_ws("|", allColumns: _*))
          val finalDataDf=dataDF.select("Final").withColumn("index", lit("2"))
          dataDF.show(200,false)
          finalDataDf.show(200,false)
        //finalDataDf.coalesce(1).write.mode("append").format("text").save(s"/tmp/ccx_dm_test/testing/responsefile/$file_name")
       // val dataDF=dataMemDf.map(x => (x(5)+"|"+ x(2)+"|"+x(0)+"|"+ x(3).toString().replace(" ","T")+"|"+ x(4)))
       // val dataSDf=spark.read.text(s"/tmp/ccx_dm_test/testing/responsefile/$file_name").withColumn("index", lit("2")).cache()
         // dataSDf.show(200,false)
          val trailerDf =trailer1Df.map(x=>(x(0)+"|"+x(1))).toDF("value").withColumn("index", lit("3"))
         trailerDf.show(200,false)
         val finalDF = headerDf.union(finalDataDf).union(trailerDf).orderBy($"index").drop($"index").coalesce(1)
        // val finalDF = headerDf.union(trailerDf).orderBy($"index").drop($"index").coalesce(1)
          finalDF.show(200,false)
        
          val curr_dt=getCurrentDate()
          var hdfpath= ConfigFactory.load.getString("migration.tracking.udhfile.hdfsPath")
          logger.info(s"### path:/tmp/ccx_dm_test/testing/responsefile/$file_name ###")
           finalDF.coalesce(1).write.mode("append").format("text").save(hdfpath+s"$file_name")
            //finalDF.coalesce(1).write.mode("append").format("text").save(s"/tmp/ccx_dm_test/testing/responsefile/$file_name)
             val hadoopConfig = new Configuration() 
           val fs = FileSystem.get(hadoopConfig)
           val srcDir = new Path(hdfpath+s"$file_name")
          val dstFile = new Path(hdfpath+s"$file_name"+".txt")
           //val srcDir = new Path(s"/tmp/ccx_dm_test/testing/responsefile/$file_name")
          //val dstFile = new Path(s"/tmp/ccx_dm_test/testing/responsefile/$file_name"+".txt")
      logger.info(s"### Outbound files created at:$srcDir dstFile:$dstFile ###")
                  if (fs.exists(srcDir)) {
            copyMerge(fs, srcDir, fs, dstFile, true, hadoopConfig)
                  }
          logger.info(s"### UDH file extraction merge ends ####")
      
      }
     else {
         logger.info("### Member Tracking table update with out errors ###")
      spark.sql(s"drop table if exists $MemtempTblNm")
      spark.sql(s"create table $MemtempTblNm like $MemtrackingTbl")
       spark.sql(s"drop table if exists  $MemtrctempTblNm")
      var MemDf=spark.sql(s"create table $MemtrctempTblNm as select gdfid from $MemtrackingTbl where status ='sent' ")
     val FileDF = spark.sql(s"select fileid,extract_file_name from  $FiletrackingTbl where status='success' ")
     val MemDF = spark.sql(s"select * from $MemtrackingTbl where status='sent' ")
    // val MemFileDF=MemDF.alias("d1").join(FileDF.alias("d2"),col("d1.fileid") === col("d2.fileid"))
     spark.sql(s"SET hive.exec.dynamic.partition = true")
      spark.sql(s"SET hive.exec.dynamic.partition.mode = nonstrict ")
      spark.sql(s"SET hive.exec.max.dynamic.partitions.pernode = 400")
     spark.sql(s"insert into $MemtempTblNm select a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime,'success' AS status,a.reasoncd as reasoncd,a.errorrcd  as errorrcd,a.run_date as run_date FROM  $MemtrackingTbl a inner JOIN  $FiletrackingTbl b ON A.fileid = B.fileid where b.status ='success' and a.status ='sent' ")
       spark.sql(s"insert into $MemtempTblNm select a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime,a.status AS status,a.reasoncd as reasoncd,a.errorrcd  as errorrcd,a.run_date as run_date FROM $MemtrackingTbl  a where a.status not in ('sent') ")
     spark.sql(s"insert into $MemtempTblNm select a.* from $MemtrackingTbl a left outer join $MemtempTblNm b on a.gdfid=b.gdfid where b.gdfid is null ")
      val MemberDF=spark.sql(s"insert overwrite table $MemtrackingTbl select * from $MemtempTblNm")
       logger.info(s"### Member Tracking table updated####")
      spark.sql(s"drop table if exists  $MemsucctempTblNm")
      logger.info(s"### UDH file extraction for Member started ####")
        logger.info(s"### UDH file extraction for Header Member started ####")
        var df =spark.sql(s"select count(*) from $MemtrackingTbl ")
      df =df.withColumn("tranTimeStamp", to_utc_timestamp(current_timestamp(), "EST"))
      val timestamp=df.select(s"tranTimeStamp")
       // val headerDf=df.map(x=> ("HDR"+"|"+"DM_UDH_MCResp_Member_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".","")+".txt"+"|"+x(1).toString().replace(" ","T")+"|" +"DATA-MIGRATION"+"|"+"UDH"+"|"+"MEMBER")).toDF("value").withColumn("index", lit("1"))
          logger.info(s"### UDH file extraction for Header Member ends ####")
          logger.info(s"### UDH file extraction for Member data started ####")
      var MemSuccessDf=spark.sql(s"create table $MemsucctempTblNm as select gdfid as successgdfid from $MemtrackingTbl where status ='success' ")
      //ErrorupdatedDF.show(200,false)
      //var MemSuccessDf=spark.sql(s"select gdfid as successgdfid from prod.dm_member_tracking_divya where status ='success' ")
     // var MemgdfidDf=MemSuccessDf.alias("d1").join(MemDf.alias("d2"),col("d1.successgdfid")=== col("d2.gdfid"),"inner")
      var MemgdfidDf=spark.sql(s"select a.successgdfid from  $MemsucctempTblNm a inner join $MemtrctempTblNm b on a.successgdfid=b.gdfid")
      MemgdfidDf.show(200,false)
     var dataMemDf= MemgdfidDf.select(s"successgdfid")
     
     var reDf=responseDf.select("FILE_NM")
     var  Dm1Df=reDf.withColumn("vendorResponse",expr("substring(FILE_NM,16,12)")).drop("FILE_NM")
     val tidf=Dm1Df.withColumn("datetype_timestamp",to_utc_timestamp(to_timestamp(col("vendorResponse"),"yyyyMMddHHmm"),"EST"))
      // val tidf =Dm1Df.withColumn("vendorResponsetimestamp", to_utc_timestamp(col("vendorResponse"), "EST")).drop("vendorResponse")
       tidf.show(200,false)
       
       val dmdsdf =tidf.map(x=> x(1).toString().replace(" ","T")).toDF("vendortimestamp")
          val dmd=dmdsdf.select("vendortimestamp").collectAsList().get(0).get(0).toString()
      // val ven=dmd
      //    dataMemDf=Dm1Df.select("vendorResponsetimestamp")
          //val tme=dmd
       dataMemDf=dataMemDf.withColumn("vendorResponsetimestamp", lit(s"$dmd"))
          dataMemDf=dataMemDf.withColumn("vendorName",lit("MEDCOMPASS")).withColumn("vendorRecordId",lit("")).withColumn("recordtype",lit("DTL"))
         // val dataDf=dataMemDf.select(s"vendorResponsetimestamp").toString()
          //val dataDF=dataMemDf.map(x => (x(5)+"|"+ x(2)+"|"+x(0)+"|"+ x(3).toString().replace(" ","T")+"|"+ x(4))).toDF("value").withColumn("index", lit("2"))
          //  val totalRows = dataDF.count()
          logger.info(s"### UDH file extraction for Member data ended ####")
      logger.info(s"### UDH file extraction for trailer Member started ####")
           val trailer =MemgdfidDf.count().toString()
           var trailer1Df = spark.sql(s"select count(*) as cnt from $MemtrackingTbl where status ='success' ")
      trailer1Df=trailer1Df.withColumn("recordType",lit("TRL")).withColumn("count",lit(trailer)).drop("cnt")
     // val trailerDf =trailer1Df.map(x=>(x(0)+"|"+x(1))).toDF("value").withColumn("index", lit("3"))
        logger.info(s"### UDH file extraction for trailer Member ends ####")
         logger.info(s"### UDH file extraction merge starts ####")
          val headerDf=df.map(x=> ("HDR"+"|"+"DM_UDH_MCResp_Member_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".","")+".txt"+"|"+x(1).toString().replace(" ","T")+"|" +"DATA-MIGRATION"+"|"+"UDH"+"|"+"MEMBER")).toDF("value").withColumn("index", lit("1"))
         headerDf.show(200,false)
           val filenameDf=df.map(x=> ("HDR","DM_UDH_MCResp_Member_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".",""),x(1).toString().replace(" ","T"),"DATA-MIGRATION","UDH","MEMBER"))
          val file_name=filenameDf.select("_2").collectAsList().get(0).get(0).toString()
           
           val sedf= dataMemDf.select("recordtype","vendorName","successgdfid","vendorResponsetimestamp","vendorRecordId")
         //  val dataDF=sedf.map(x =>(x(0)+"|"+ x(1)+"|"+x(2)+"|"+ x(3)+"|"x(4)))
           val allColumns = sedf.columns.map(x => col(x))
         val dataDF=sedf.withColumn("Final", concat_ws("|", allColumns: _*))
          val finalDataDf=dataDF.select("Final").withColumn("index", lit("2"))
          dataDF.show(200,false)
          finalDataDf.show(200,false)
        //finalDataDf.coalesce(1).write.mode("append").format("text").save(s"/tmp/ccx_dm_test/testing/responsefile/$file_name")
       // val dataDF=dataMemDf.map(x => (x(5)+"|"+ x(2)+"|"+x(0)+"|"+ x(3).toString().replace(" ","T")+"|"+ x(4)))
       // val dataSDf=spark.read.text(s"/tmp/ccx_dm_test/testing/responsefile/$file_name").withColumn("index", lit("2")).cache()
         // dataSDf.show(200,false)
          val trailerDf =trailer1Df.map(x=>(x(0)+"|"+x(1))).toDF("value").withColumn("index", lit("3"))
         trailerDf.show(200,false)
         val finalDF = headerDf.union(finalDataDf).union(trailerDf).orderBy($"index").drop($"index").coalesce(1)
        // val finalDF = headerDf.union(trailerDf).orderBy($"index").drop($"index").coalesce(1)
          finalDF.show(200,false)
        
          val curr_dt=getCurrentDate()
          var hdfpath= ConfigFactory.load.getString("migration.tracking.udhfile.hdfsPath")
          logger.info(s"### path:/tmp/ccx_dm_test/testing/responsefile/$file_name ###")
           finalDF.coalesce(1).write.mode("append").format("text").save(hdfpath+s"$file_name")
            //finalDF.coalesce(1).write.mode("append").format("text").save(s"/tmp/ccx_dm_test/testing/responsefile/$file_name)
             val hadoopConfig = new Configuration() 
           val fs = FileSystem.get(hadoopConfig)
           val srcDir = new Path(hdfpath+s"$file_name")
          val dstFile = new Path(hdfpath+s"$file_name"+".txt")
           //val srcDir = new Path(s"/tmp/ccx_dm_test/testing/responsefile/$file_name")
          //val dstFile = new Path(s"/tmp/ccx_dm_test/testing/responsefile/$file_name"+".txt")
      logger.info(s"### Outbound files created at:$srcDir dstFile:$dstFile ###")
                  if (fs.exists(srcDir)) {
            copyMerge(fs, srcDir, fs, dstFile, true, hadoopConfig)
                  }
          logger.info(s"### UDH file extraction merge ends ####")
      
      }
     logger.info("### Provider Tracking table update###")
      if (!isDataFrameEmpty(ErrorproDF)) {
     spark.sql(s"drop table if exists $PrvtempTblNm")
      spark.sql(s"create table $PrvtempTblNm like $PrvtrackingTbl")
      //spark.sql(s"create table $PrvtempTblNm like $MemtrackingTbl")
       spark.sql(s"drop table if exists  $PrvtrctempTblNm")
      var PrvDf=spark.sql(s"create table $PrvtrctempTblNm as select gdfid from $PrvtrackingTbl where status ='sent' ")
      spark.sql(s"SET hive.exec.dynamic.partition = true")
      spark.sql(s"SET hive.exec.dynamic.partition.mode = nonstrict ")
      spark.sql(s"SET hive.exec.max.dynamic.partitions.pernode = 400")
      spark.sql(s"INSERT INTO table $PrvtempTblNm SELECT a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime,'failed'  AS status,b.reasoncd as reasoncd, b.errorcd  as errorrcd,a.run_date as run_date FROM $PrvErrtempTblNm b LEFT OUTER JOIN  $PrvtrackingTbl a ON A.gdfid = B.gdfid") 
      spark.sql(s"INSERT INTO table $PrvtempTblNm  SELECT a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime,a.status AS status,'' as reasoncd, '' as errorrcd,a.run_date as run_date FROM $PrvtrackingTbl a LEFT OUTER JOIN $PrvErrtempTblNm b ON A.gdfid = B.gdfid where b.gdfid is null and a.status not in ('sent')")
      spark.sql(s" INSERT INTO table $PrvtempTblNm  SELECT a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime,'success' AS status,'' as reasoncd, '' as errorrcd,a.run_date as run_date FROM  $PrvtrackingTbl a LEFT OUTER JOIN  $PrvErrtempTblNm b ON A.gdfid = B.gdfid where b.gdfid is null and a.status='sent'")
      var ErrorpupdatedDF=spark.sql(s"insert overwrite table $PrvtrackingTbl select * from $PrvtempTblNm")
      logger.info(s"### Provider Tracking table updated####")
      spark.sql(s"drop table if exists  $PrvsucctempTblNm")
      logger.info(s"### UDH file extraction for Provider started ####")
        logger.info(s"### UDH file extraction for Header Provider started ####")
        var df =spark.sql(s"select count(*) from $PrvtrackingTbl ")
      df =df.withColumn("tranTimeStamp", to_utc_timestamp(current_timestamp(), "EST"))
      val timestamp=df.select(s"tranTimeStamp")
       // val headerDf=df.map(x=> ("HDR"+"|"+"DM_UDH_MCResp_Member_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".","")+".txt"+"|"+x(1).toString().replace(" ","T")+"|" +"DATA-MIGRATION"+"|"+"UDH"+"|"+"MEMBER")).toDF("value").withColumn("index", lit("1"))
          logger.info(s"### UDH file extraction for Header Provider ends ####")
          logger.info(s"### UDH file extraction for Provider data started ####")
      var PrvSuccessDf=spark.sql(s"create table $PrvsucctempTblNm as select gdfid as successgdfid from $PrvtrackingTbl where status ='success' ")
      //ErrorupdatedDF.show(200,false)
      //var MemSuccessDf=spark.sql(s"select gdfid as successgdfid from prod.dm_member_tracking_divya where status ='success' ")
     // var MemgdfidDf=MemSuccessDf.alias("d1").join(MemDf.alias("d2"),col("d1.successgdfid")=== col("d2.gdfid"),"inner")
      var PrvgdfidDf=spark.sql(s"select a.successgdfid from  $PrvsucctempTblNm a inner join $PrvtrctempTblNm b on a.successgdfid=b.gdfid")
      PrvgdfidDf.show(200,false)
     var dataMemDf= PrvgdfidDf.select(s"successgdfid")
     
     var reDf=responseDf.select("FILE_NM")
     var  Dm1Df=reDf.withColumn("vendorResponse",expr("substring(FILE_NM,16,12)")).drop("FILE_NM")
     val tidf=Dm1Df.withColumn("datetype_timestamp",to_utc_timestamp(to_timestamp(col("vendorResponse"),"yyyyMMddHHmm"),"EST"))
      // val tidf =Dm1Df.withColumn("vendorResponsetimestamp", to_utc_timestamp(col("vendorResponse"), "EST")).drop("vendorResponse")
       tidf.show(200,false)
       
       val dmdsdf =tidf.map(x=> x(1).toString().replace(" ","T")).toDF("vendortimestamp")
          val dmd=dmdsdf.select("vendortimestamp").collectAsList().get(0).get(0).toString()
      // val ven=dmd
      //    dataMemDf=Dm1Df.select("vendorResponsetimestamp")
          //val tme=dmd
       dataMemDf=dataMemDf.withColumn("vendorResponsetimestamp", lit(s"$dmd"))
          dataMemDf=dataMemDf.withColumn("vendorName",lit("MEDCOMPASS")).withColumn("vendorRecordId",lit("")).withColumn("recordtype",lit("DTL"))
         // val dataDf=dataMemDf.select(s"vendorResponsetimestamp").toString()
          //val dataDF=dataMemDf.map(x => (x(5)+"|"+ x(2)+"|"+x(0)+"|"+ x(3).toString().replace(" ","T")+"|"+ x(4))).toDF("value").withColumn("index", lit("2"))
          //  val totalRows = dataDF.count()
          logger.info(s"### UDH file extraction for Provider data ended ####")
      logger.info(s"### UDH file extraction for trailer Provider started ####")
           val trailer =PrvgdfidDf.count().toString()
           var trailer1Df = spark.sql(s"select count(*) as cnt from $MemtrackingTbl where status ='success' ")
      trailer1Df=trailer1Df.withColumn("recordType",lit("TRL")).withColumn("count",lit(trailer)).drop("cnt")
     // val trailerDf =trailer1Df.map(x=>(x(0)+"|"+x(1))).toDF("value").withColumn("index", lit("3"))
        logger.info(s"### UDH file extraction for trailer Provider ends ####")
         logger.info(s"### UDH file extraction merge starts ####")
          val headerDf=df.map(x=> ("HDR"+"|"+"DM_UDH_MCResp_Provider_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".","")+".txt"+"|"+x(1).toString().replace(" ","T")+"|" +"DATA-MIGRATION"+"|"+"UDH"+"|"+"PROVIDER")).toDF("value").withColumn("index", lit("1"))
         headerDf.show(200,false)
           val filenameDf=df.map(x=> ("HDR","DM_UDH_MCResp_Provider_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".",""),x(1).toString().replace(" ","T"),"DATA-MIGRATION","UDH","PROVIDER"))
          val file_name=filenameDf.select("_2").collectAsList().get(0).get(0).toString()
           
           val sedf= dataMemDf.select("recordtype","vendorName","successgdfid","vendorResponsetimestamp","vendorRecordId")
         //  val dataDF=sedf.map(x =>(x(0)+"|"+ x(1)+"|"+x(2)+"|"+ x(3)+"|"x(4)))
           val allColumns = sedf.columns.map(x => col(x))
         val dataDF=sedf.withColumn("Final", concat_ws("|", allColumns: _*))
          val finalDataDf=dataDF.select("Final").withColumn("index", lit("2"))
          dataDF.show(200,false)
          finalDataDf.show(200,false)
        //finalDataDf.coalesce(1).write.mode("append").format("text").save(s"/tmp/ccx_dm_test/testing/responsefile/$file_name")
       // val dataDF=dataMemDf.map(x => (x(5)+"|"+ x(2)+"|"+x(0)+"|"+ x(3).toString().replace(" ","T")+"|"+ x(4)))
       // val dataSDf=spark.read.text(s"/tmp/ccx_dm_test/testing/responsefile/$file_name").withColumn("index", lit("2")).cache()
         // dataSDf.show(200,false)
          val trailerDf =trailer1Df.map(x=>(x(0)+"|"+x(1))).toDF("value").withColumn("index", lit("3"))
         trailerDf.show(200,false)
         val finalDF = headerDf.union(finalDataDf).union(trailerDf).orderBy($"index").drop($"index").coalesce(1)
        // val finalDF = headerDf.union(trailerDf).orderBy($"index").drop($"index").coalesce(1)
          finalDF.show(200,false)
        
          val curr_dt=getCurrentDate()
          var hdfpath= ConfigFactory.load.getString("migration.tracking.udhfile.hdfsPath")
          logger.info(s"### path:/tmp/ccx_dm_test/testing/responsefile/$file_name ###")
           finalDF.coalesce(1).write.mode("append").format("text").save(hdfpath+s"$file_name")
            //finalDF.coalesce(1).write.mode("append").format("text").save(s"/tmp/ccx_dm_test/testing/responsefile/$file_name)
             val hadoopConfig = new Configuration() 
           val fs = FileSystem.get(hadoopConfig)
           val srcDir = new Path(hdfpath+s"$file_name")
          val dstFile = new Path(hdfpath+s"$file_name"+".txt")
           //val srcDir = new Path(s"/tmp/ccx_dm_test/testing/responsefile/$file_name")
          //val dstFile = new Path(s"/tmp/ccx_dm_test/testing/responsefile/$file_name"+".txt")
      logger.info(s"### Outbound files created at:$srcDir dstFile:$dstFile ###")
                  if (fs.exists(srcDir)) {
            copyMerge(fs, srcDir, fs, dstFile, true, hadoopConfig)
                  }
          logger.info(s"### UDH file extraction merge ends ####")
      
      }
        else {
         logger.info("### Provider Tracking table update with out errors ###")
      spark.sql(s"drop table if exists $PrvtempTblNm")
      spark.sql(s"create table $PrvtempTblNm like $PrvtrackingTbl")
     val FileDF = spark.sql(s"select fileid,extract_file_name from  $FiletrackingTbl where status='success' ")
     val PrvDF = spark.sql(s"select * from $PrvtrackingTbl where status='sent' ")
     spark.sql(s"drop table if exists  $PrvtrctempTblNm")
      var PrvDf=spark.sql(s"create table $PrvtrctempTblNm as select gdfid from $PrvtrackingTbl where status ='sent' ")
    // val MemFileDF=MemDF.alias("d1").join(FileDF.alias("d2"),col("d1.fileid") === col("d2.fileid"))
      spark.sql(s"SET hive.exec.dynamic.partition = true")
      spark.sql(s"SET hive.exec.dynamic.partition.mode = nonstrict ")
      spark.sql(s"SET hive.exec.max.dynamic.partitions.pernode = 400")
     spark.sql(s"insert into $PrvtempTblNm select b.fileid as fileid,b.gdfid as gdfid,b.RunTime as RunTime,'success' AS status,b.reasoncd as reasoncd, b.errorrcd  as errorrcd,b.run_date as run_date FROM $PrvtrackingTbl  b inner JOIN  $FiletrackingTbl  a ON A.fileid = B.fileid where b.status ='success' and a.status ='sent'")
       spark.sql(s"insert into $PrvtempTblNm select a.fileid as fileid,a.gdfid as gdfid,a.RunTime as RunTime,a.status AS status,a.reasoncd as reasoncd, a.errorrcd  as errorrcd,a.run_date as run_date FROM $PrvtrackingTbl  a where a.status not in ('sent') ")
     spark.sql(s"insert into  $PrvtempTblNm select a.* from $PrvtrackingTbl a left outer join $PrvtempTblNm b on a.gdfid=b.gdfid where a.gdfid is null ")
      val ProvDF=spark.sql(s"insert overwrite table $PrvtrackingTbl select * from $PrvtempTblNm")
      logger.info(s"### Provider Tracking table updated####")
      spark.sql(s"drop table if exists  $PrvsucctempTblNm")
      logger.info(s"### UDH file extraction for Provider started ####")
        logger.info(s"### UDH file extraction for Header Provider started ####")
        var df =spark.sql(s"select count(*) from $PrvtrackingTbl ")
      df =df.withColumn("tranTimeStamp", to_utc_timestamp(current_timestamp(), "EST"))
      val timestamp=df.select(s"tranTimeStamp")
       // val headerDf=df.map(x=> ("HDR"+"|"+"DM_UDH_MCResp_Member_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".","")+".txt"+"|"+x(1).toString().replace(" ","T")+"|" +"DATA-MIGRATION"+"|"+"UDH"+"|"+"MEMBER")).toDF("value").withColumn("index", lit("1"))
          logger.info(s"### UDH file extraction for Header Provider ends ####")
          logger.info(s"### UDH file extraction for Provider data started ####")
      var PrvSuccessDf=spark.sql(s"create table $PrvsucctempTblNm as select gdfid as successgdfid from $PrvtrackingTbl where status ='success' ")
      //ErrorupdatedDF.show(200,false)
      //var MemSuccessDf=spark.sql(s"select gdfid as successgdfid from prod.dm_member_tracking_divya where status ='success' ")
     // var MemgdfidDf=MemSuccessDf.alias("d1").join(MemDf.alias("d2"),col("d1.successgdfid")=== col("d2.gdfid"),"inner")
      var PrvgdfidDf=spark.sql(s"select a.successgdfid from  $PrvsucctempTblNm a inner join $PrvtrctempTblNm b on a.successgdfid=b.gdfid")
      PrvgdfidDf.show(200,false)
     var dataMemDf= PrvgdfidDf.select(s"successgdfid")
     
     var reDf=responseDf.select("FILE_NM")
     var  Dm1Df=reDf.withColumn("vendorResponse",expr("substring(FILE_NM,16,12)")).drop("FILE_NM")
     val tidf=Dm1Df.withColumn("datetype_timestamp",to_utc_timestamp(to_timestamp(col("vendorResponse"),"yyyyMMddHHmm"),"EST"))
      // val tidf =Dm1Df.withColumn("vendorResponsetimestamp", to_utc_timestamp(col("vendorResponse"), "EST")).drop("vendorResponse")
       tidf.show(200,false)
       
       val dmdsdf =tidf.map(x=> x(1).toString().replace(" ","T")).toDF("vendortimestamp")
          val dmd=dmdsdf.select("vendortimestamp").collectAsList().get(0).get(0).toString()
      // val ven=dmd
      //    dataMemDf=Dm1Df.select("vendorResponsetimestamp")
          //val tme=dmd
       dataMemDf=dataMemDf.withColumn("vendorResponsetimestamp", lit(s"$dmd"))
          dataMemDf=dataMemDf.withColumn("vendorName",lit("MEDCOMPASS")).withColumn("vendorRecordId",lit("")).withColumn("recordtype",lit("DTL"))
         // val dataDf=dataMemDf.select(s"vendorResponsetimestamp").toString()
          //val dataDF=dataMemDf.map(x => (x(5)+"|"+ x(2)+"|"+x(0)+"|"+ x(3).toString().replace(" ","T")+"|"+ x(4))).toDF("value").withColumn("index", lit("2"))
          //  val totalRows = dataDF.count()
          logger.info(s"### UDH file extraction for Provider data ended ####")
      logger.info(s"### UDH file extraction for trailer Provider started ####")
           val trailer =PrvgdfidDf.count().toString()
           var trailer1Df = spark.sql(s"select count(*) as cnt from $MemtrackingTbl where status ='success' ")
      trailer1Df=trailer1Df.withColumn("recordType",lit("TRL")).withColumn("count",lit(trailer)).drop("cnt")
     // val trailerDf =trailer1Df.map(x=>(x(0)+"|"+x(1))).toDF("value").withColumn("index", lit("3"))
        logger.info(s"### UDH file extraction for trailer Provider ends ####")
         logger.info(s"### UDH file extraction merge starts ####")
          val headerDf=df.map(x=> ("HDR"+"|"+"DM_UDH_MCResp_Provider_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".","")+".txt"+"|"+x(1).toString().replace(" ","T")+"|" +"DATA-MIGRATION"+"|"+"UDH"+"|"+"PROVIDER")).toDF("value").withColumn("index", lit("1"))
         headerDf.show(200,false)
           val filenameDf=df.map(x=> ("HDR","DM_UDH_MCResp_Provider_"+x(1).toString().replace(" ","T").replace("-","").replace(":","").replace(".",""),x(1).toString().replace(" ","T"),"DATA-MIGRATION","UDH","PROVIDER"))
          val file_name=filenameDf.select("_2").collectAsList().get(0).get(0).toString()
           
           val sedf= dataMemDf.select("recordtype","vendorName","successgdfid","vendorResponsetimestamp","vendorRecordId")
         //  val dataDF=sedf.map(x =>(x(0)+"|"+ x(1)+"|"+x(2)+"|"+ x(3)+"|"x(4)))
           val allColumns = sedf.columns.map(x => col(x))
         val dataDF=sedf.withColumn("Final", concat_ws("|", allColumns: _*))
          val finalDataDf=dataDF.select("Final").withColumn("index", lit("2"))
          dataDF.show(200,false)
          finalDataDf.show(200,false)
        //finalDataDf.coalesce(1).write.mode("append").format("text").save(s"/tmp/ccx_dm_test/testing/responsefile/$file_name")
       // val dataDF=dataMemDf.map(x => (x(5)+"|"+ x(2)+"|"+x(0)+"|"+ x(3).toString().replace(" ","T")+"|"+ x(4)))
       // val dataSDf=spark.read.text(s"/tmp/ccx_dm_test/testing/responsefile/$file_name").withColumn("index", lit("2")).cache()
         // dataSDf.show(200,false)
          val trailerDf =trailer1Df.map(x=>(x(0)+"|"+x(1))).toDF("value").withColumn("index", lit("3"))
         trailerDf.show(200,false)
         val finalDF = headerDf.union(finalDataDf).union(trailerDf).orderBy($"index").drop($"index").coalesce(1)
        // val finalDF = headerDf.union(trailerDf).orderBy($"index").drop($"index").coalesce(1)
          finalDF.show(200,false)
        
          val curr_dt=getCurrentDate()
          var hdfpath= ConfigFactory.load.getString("migration.tracking.udhfile.hdfsPath")
          logger.info(s"### path:/tmp/ccx_dm_test/testing/responsefile/$file_name ###")
           finalDF.coalesce(1).write.mode("append").format("text").save(hdfpath+s"$file_name")
            //finalDF.coalesce(1).write.mode("append").format("text").save(s"/tmp/ccx_dm_test/testing/responsefile/$file_name)
             val hadoopConfig = new Configuration() 
           val fs = FileSystem.get(hadoopConfig)
           val srcDir = new Path(hdfpath+s"$file_name")
          val dstFile = new Path(hdfpath+s"$file_name"+".txt")
           //val srcDir = new Path(s"/tmp/ccx_dm_test/testing/responsefile/$file_name")
          //val dstFile = new Path(s"/tmp/ccx_dm_test/testing/responsefile/$file_name"+".txt")
      logger.info(s"### Outbound files created at:$srcDir dstFile:$dstFile ###")
                  if (fs.exists(srcDir)) {
            copyMerge(fs, srcDir, fs, dstFile, true, hadoopConfig)
                  }
          logger.info(s"### UDH file extraction merge ends ####")
      
      }
     
     
       logger.info("### ServiceAuth Tracking table update###")
       if (!isDataFrameEmpty(ErrorsaDF)) {
      spark.sql(s"drop table if exists $SAtempTblNm")
      spark.sql(s"create table $SAtempTblNm like $SAtrackingTbl")
      spark.sql(s"SET hive.exec.dynamic.partition = true")
      spark.sql(s"SET hive.exec.dynamic.partition.mode = nonstrict ")
      spark.sql(s"SET hive.exec.max.dynamic.partitions.pernode = 400")
      spark.sql(s"INSERT INTO table $SAtempTblNm  SELECT a.fileid as fileid,a.caseid as caseid,a.planid as planid,a.RunTime as RunTime,'failed'  AS status,b.reasoncd as reasoncd, b.errorcd  as errorrcd,a.run_date as run_date FROM $SAErrtempTblNm  b LEFT OUTER JOIN  $SAtrackingTbl a ON A.caseid = B.caseid") 
      spark.sql(s"INSERT INTO table $SAtempTblNm  SELECT a.fileid as fileid,a.caseid as caseid,a.planid as planid,a.RunTime as RunTime,a.status AS status,'' as reasoncd, '' as errorrcd,a.run_date as run_date FROM  $SAtrackingTbl a LEFT OUTER JOIN  $SAErrtempTblNm b ON A.caseid = B.caseid where b.caseid is null and a.status not in ('sent')")
      spark.sql(s" INSERT INTO table $SAtempTblNm  SELECT a.fileid as fileid,a.caseid as caseid,a.planid as planid,a.RunTime as RunTime,'success' AS status,'' as reasoncd, '' as errorrcd,a.run_date as run_date FROM  $SAtrackingTbl a LEFT OUTER JOIN  $SAErrtempTblNm b ON A.caseid = B.caseid where b.caseid is null and a.status='sent'")
      var ErrorsupdatedDF=spark.sql(s"insert overwrite table $SAtrackingTbl select * from $SAtempTblNm")
       }
      else {
         logger.info("### ServiceAuth Tracking table update with out errors ###")
      spark.sql(s"drop table if exists $SAtempTblNm")
      spark.sql(s"create table $SAtempTblNm like $SAtrackingTbl")
     val FileDF = spark.sql(s"select fileid,extract_file_name from $FiletrackingTbl where status='success' ")
     //val saDF = spark.sql(s"select * from prod.dm_provider_tracking where status='sent' ")
    // val MemFileDF=MemDF.alias("d1").join(FileDF.alias("d2"),col("d1.fileid") === col("d2.fileid"))
     spark.sql(s"SET hive.exec.dynamic.partition = true")
      spark.sql(s"SET hive.exec.dynamic.partition.mode = nonstrict ")
      spark.sql(s"SET hive.exec.max.dynamic.partitions.pernode = 400")
     spark.sql(s"insert into $SAtempTblNm select b.fileid as fileid,b.caseid as caseid,b.planid as planid,b.RunTime as RunTime,'success' AS status,b.reasoncd as reasoncd, b.errorrcd  as errorrcd,b.run_date as run_date FROM $SAtrackingTbl  b inner JOIN  $FiletrackingTbl a ON A.fileid = B.fileid where b.status ='success' and a.status ='sent'")
       spark.sql(s"insert into $SAtempTblNm select a.fileid as fileid,a.caseid as caseid, a.planid as planid,a.RunTime as RunTime,a.status AS status,a.reasoncd as reasoncd, a.errorrcd  as errorrcd,a.run_date as run_date FROM $SAtrackingTbl a where a.status not in ('sent') ")
    spark.sql(s" insert into $SAtempTblNm select a.* from $SAtempTblNm a left outer join $SAtempTblNm b a.caseid=b.caseid where a.caseid is null ")
      val sasDF=spark.sql(s"insert overwrite table $SAtrackingTbl select * from $SAtempTblNm")
      }
      
      
    } catch {
      case e:Exception => {
            logger.error(e)
            throw(e)
      }
    }        
  }
}
