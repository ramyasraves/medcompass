package ccx.dmAudit
import java.io.{PrintWriter, StringWriter}
import java.util.Properties

import org.apache.spark.sql.functions._
import org.apache.log4j.{LogManager, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.types.{IntegerType, StringType, StructType}

object auditDriver {
  @transient lazy val logger: Logger = LogManager.getLogger("Data Migration")

  def main(args: Array[String]): Unit = {
    try{
    import org.apache.log4j.{LogManager, Logger}

    var mode = ""
    var sessionName = ""
    var logfileName = ""
    var propertyFile = ""
    // Pass below arguments for the spark submit in order:
    //	1. Mode of Spark execution - Local/cluster
    //  2. Spark Session name
    //  3. Configuration File - CSV for source to target mapping including transformation details
    //  4. Property file - contains the common params
    if (args.size == 3) {
      mode = args(0)
      logfileName = args(1)
      propertyFile = args(2)
      //propertyFile = args(3)
      logger.info("### Starting the job ###")
      logger.info("==================================================")
      logger.info(s"### Running mode : $mode ###")
      logger.info(s"### Spark session name : $logfileName ###")
      logger.info(s"### Property File   : $propertyFile ###")
      logger.info("==================================================")
    } else {
      logger.info("### Please provide the required arguments ###")
      System.exit(1)
    }

    //Read Properties file
    val properties : Properties = new Properties()
    val source = scala.io.Source.fromFile(propertyFile)
    if (source != null) {
      properties.load(source.bufferedReader())
    }
    else {
      logger.error("properties file not valid")
      sys.exit()
    }
    //Process logfile from argument
    val spark = SparkSession
      .builder
      .appName("ccx-mc-dm-audit")
      .master(mode)
      .getOrCreate()

    val logSchema = new StructType()
      .add("objFilename", StringType, true)
      .add("objTimestamp",StringType,true)
      .add("objName", StringType, true)
      .add("objRecCount", IntegerType, true)

    val logPath=properties.getProperty("dm_log_hdfs_path") + logfileName
    val logDF= spark.read.option("delimiter","|").schema(logSchema).csv(logPath)

    val dboptions: Map[String, String] =
      Map(
        "URL" -> properties.getProperty("migration.tracking.oracle.serverurl"),
        "driver" -> properties.getProperty("migration.tracking.oracle.jdbcdriver"),
        "user" -> properties.getProperty("migration.tracking.oracle.userid"),
        "password" -> properties.getProperty("migration.tracking.oracle.password"),
        "dbtable" -> properties.getProperty("migration.tracking.oracle.dbtable"))

      val totalCntDF=logDF.groupBy(col("objFilename")).agg(sum(col("objRecCount")).as("totalRecCount"))

      totalCntDF.show(false)

      val tempAuditDF=logDF.alias("d1").join(totalCntDF.alias("d2"),col("d1.objFilename")===col("d2.objFilename"))
        .withColumn("desc1",concat_ws(":",lit("Total Count"),col("totalRecCount")))
        .withColumn("desc2",concat_ws(":",col("objName"),col("objRecCount")))
        .select(col("d2.objFilename"),col("d2.totalRecCount"),col("desc1"),col("desc2"),col("d1.objTimestamp"))
        .groupBy("d2.objFilename","d1.objTimestamp","desc1")
        .agg(collect_list("desc2").as("desc2"))
        .withColumn("desc",concat_ws("|",col("desc1"),col("desc2")))
        .withColumn("index",row_number().over(Window.orderBy(col("objFileName").asc)))
        .select("objFilename","objTimestamp","desc","index")

      var sqlMax = "SELECT COUNT(*) FROM " + properties.getProperty("migration.tracking.oracle.dbtable")
      sqlMax = ("(").concat(sqlMax).concat(") T")

      val dboptions1: Map[String, String] =
        Map(
          "URL" -> properties.getProperty("migration.tracking.oracle.serverurl"),
          "driver" -> properties.getProperty("migration.tracking.oracle.jdbcdriver"),
          "user" -> properties.getProperty("migration.tracking.oracle.userid"),
          "password" -> properties.getProperty("migration.tracking.oracle.password"),
          "dbtable" -> sqlMax)

      var MIGRATION_TRACKING_ID: Long = 0
      var maxTrkId = spark.read.format("jdbc").options(dboptions1).load.first().getDecimal(0).longValue();
          MIGRATION_TRACKING_ID = maxTrkId
      var auditDF=tempAuditDF.withColumn("MIGRATION_TRKNG_ID",col("index") + MIGRATION_TRACKING_ID)
        .withColumn("FILE_NM",col("objFilename"))
        .withColumn("SUMM_MSG_TYPE_DESC",lit("Outbound File Summary"))
        .withColumn("SUMM_ERROR_MSG_TXT",col("desc"))
        .select("MIGRATION_TRKNG_ID","FILE_NM", "SUMM_MSG_TYPE_DESC", "SUMM_ERROR_MSG_TXT")

      auditDF.write.format("jdbc").options(dboptions).mode("append").save()

  } catch {
      case e: Exception => {
        val sw = new StringWriter
        e.printStackTrace(new PrintWriter(sw))
        val stackTrace = sw.toString
        print("Showing the error stackTrace " + stackTrace)
      }
    }
  }
}
